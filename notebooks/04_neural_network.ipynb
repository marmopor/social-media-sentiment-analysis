{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "title_cell"
            },
            "source": [
                "# 04 - Red Neuronal Simple: Clasificaci√≥n de Sentimientos\n",
                "\n",
                "Este notebook aplica la **red neuronal m√°s simple posible (MLP de 2 capas)** para clasificar el sentimiento de posts en redes sociales.\n",
                "\n",
                "El modelo se define en `models/simple_nn.py` y se carga desde este notebook.\n",
                "\n",
                "**Dataset:** SocialBuzz Sentiment Analytics (732 muestras)\n",
                "\n",
                "**Target:** Sentimiento agrupado en 3 clases: `Positivo`, `Neutro`, `Negativo`\n",
                "\n",
                "**Arquitectura:** Input ‚Üí Linear(64) ‚Üí ReLU ‚Üí Linear(3) ‚Üí CrossEntropy"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0) Instalaci√≥n e importaci√≥n de librer√≠as"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_cell"
            },
            "outputs": [],
            "source": [
                "# Instalar kagglehub y clonar repositorio con el modelo si estamos en Colab\n",
                "import sys\n",
                "import os\n",
                "\n",
                "try:\n",
                "    import kagglehub\n",
                "except ImportError:\n",
                "    import subprocess\n",
                "    subprocess.run(['pip', 'install', 'kagglehub', '-q'])\n",
                "    import kagglehub\n",
                "\n",
                "# Detectar si estamos en Google Colab\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IN_COLAB:\n",
                "    # En Colab: montar Drive o clonar el repo de GitHub para acceder a models/simple_nn.py\n",
                "    # Opci√≥n A: clonar el repositorio del proyecto\n",
                "    # subprocess.run(['git', 'clone', 'https://github.com/TU_USUARIO/TU_REPO.git', '/content/proyecto'])\n",
                "    # sys.path.insert(0, '/content/proyecto')\n",
                "    \n",
                "    # Opci√≥n B: crear el archivo directamente (para que el notebook sea aut√≥nomo en Colab)\n",
                "    os.makedirs('models', exist_ok=True)\n",
                "    model_code = '''\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class SimpleSentimentNN(nn.Module):\n",
                "    \"\"\"\n",
                "    Red neuronal simple (MLP) para clasificacion de sentimientos.\n",
                "    Arquitectura minima: Input -> Linear(hidden) -> ReLU -> Linear(num_classes)\n",
                "    \"\"\"\n",
                "    def __init__(self, input_dim: int, hidden_dim: int = 64, num_classes: int = 3):\n",
                "        super(SimpleSentimentNN, self).__init__()\n",
                "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
                "        self.relu = nn.ReLU()\n",
                "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        out = self.fc1(x)\n",
                "        out = self.relu(out)\n",
                "        out = self.fc2(out)\n",
                "        return out\n",
                "\n",
                "    def count_parameters(self) -> int:\n",
                "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
                "\n",
                "def build_model(input_dim: int, hidden_dim: int = 64, num_classes: int = 3):\n",
                "    return SimpleSentimentNN(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes)\n",
                "'''\n",
                "    with open('models/simple_nn.py', 'w') as f:\n",
                "        f.write(model_code)\n",
                "    # A√±adir models/ al path\n",
                "    sys.path.insert(0, '.')\n",
                "    print('Archivo models/simple_nn.py creado para Colab.')\n",
                "else:\n",
                "    # Localmente: ajustar el path para encontrar el m√≥dulo\n",
                "    current_dir = os.path.dirname(os.path.abspath('__file__')) if '__file__' in dir() else os.getcwd()\n",
                "    sys.path.insert(0, current_dir)\n",
                "    print(f'Ejecutando localmente. Path: {current_dir}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "imports_cell"
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.utils import class_weight\n",
                "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "\n",
                "# Importar el modelo desde models/simple_nn.py\n",
                "from models.simple_nn import SimpleSentimentNN, build_model\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "RANDOM_STATE = 42\n",
                "torch.manual_seed(RANDOM_STATE)\n",
                "np.random.seed(RANDOM_STATE)\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Device: {device}')\n",
                "print('Librer√≠as importadas correctamente.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1) Carga de datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "load_data_cell"
            },
            "outputs": [],
            "source": [
                "path = kagglehub.dataset_download('eshummalik/socialbuzz-sentiment-analytics')\n",
                "file_path = os.path.join(path, 'sentimentdataset.csv')\n",
                "df = pd.read_csv(file_path)\n",
                "\n",
                "print(f'Shape: {df.shape}')\n",
                "df.head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2) Preprocesado"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "preprocess_cell"
            },
            "outputs": [],
            "source": [
                "df['Text'] = df['Text'].astype(str).str.strip()\n",
                "df['Sentiment'] = df['Sentiment'].astype(str).str.strip()\n",
                "\n",
                "POSITIVE_EMOTIONS = {\n",
                "    'Joy', 'Positive', 'Happiness', 'Happy', 'Excitement', 'Elation', 'Euphoria',\n",
                "    'Love', 'Gratitude', 'Contentment', 'Optimism', 'Hope', 'Hopeful', 'Satisfaction',\n",
                "    'Pride', 'Proud', 'Amusement', 'Awe', 'Inspiration', 'Inspired', 'Enthusiasm',\n",
                "    'Confidence', 'Confident', 'Empowerment', 'Freedom', 'Courage', 'Determination',\n",
                "    'Accomplishment', 'Celebration', 'Serenity', 'Tranquility', 'Peace', 'Calmness',\n",
                "    'Positivity', 'Blessed', 'Warmth', 'Heartwarming', 'Tenderness', 'Affection',\n",
                "    'Admiration', 'Adoration', 'Amazement', 'Wonder', 'Wonderment', 'Enchantment',\n",
                "    'Captivation', 'Marvel', 'Charm', 'Playful', 'PlayfulJoy', 'FestiveJoy',\n",
                "    'JoyfulReunion', 'Overjoyed', 'Ecstasy', 'Triumph', 'Success', 'Fulfillment',\n",
                "    'Appreciation', 'Relieved', 'Relief', 'Kindness', 'Kind',\n",
                "    'Compassion', 'Compassionate', 'Sympathy', 'Empathetic', 'Friendship', 'Romance',\n",
                "    'Connection', 'Harmony', 'Radiance', 'Zest', 'Energy', 'Vibrancy', 'Spark',\n",
                "    'Breakthrough', 'Motivation', 'Resilience', 'Adventure', 'Exploration',\n",
                "    'Curiosity', 'Imagination', 'Creativity', 'Creative Inspiration', 'ArtisticBurst',\n",
                "    'Grateful', 'Rejuvenation', 'Journey', 'Mindfulness', 'Solace', 'Touched',\n",
                "    'Acceptance', 'Bittersweet', 'Whimsy', 'Free-spirited',\n",
                "    'Dazzle', 'Hypnotic', 'Mesmerizing', 'Iconic', 'Melodic', 'Grandeur',\n",
                "    'Reverence', 'Anticipation', 'Thrill', 'Thrilling Journey', 'Immersion',\n",
                "    'Engagement', 'Colorful', 'Elegance', 'Runway Creativity',\n",
                "    'CulinaryOdyssey', 'Culinary Adventure', 'Joy in Baking', 'Adrenaline',\n",
                "    \"Nature's Beauty\", \"Ocean's Freedom\", 'Celestial Wonder', 'Envisioning History',\n",
                "    'Winter Magic', 'Whispers of the Past', 'Ruins', 'Enjoyment', 'Intrigue',\n",
                "    'DreamChaser', 'InnerJourney', 'Arousal',\n",
                "}\n",
                "\n",
                "NEGATIVE_EMOTIONS = {\n",
                "    'Negative', 'Sadness', 'Sad', 'Anger', 'Fear', 'Fearful', 'Despair', 'Desperation',\n",
                "    'Grief', 'Sorrow', 'Heartbreak', 'Heartache', 'LostLove', 'Loss', 'Loneliness',\n",
                "    'Isolation', 'Disappointment', 'Disappointed', 'Regret', 'Guilt', 'Shame',\n",
                "    'Frustration', 'Frustrated', 'Hate', 'Resentment', 'Envy', 'Envious',\n",
                "    'Jealousy', 'Jealous', 'Disgust', 'Betrayal', 'Bitterness', 'Bitter', 'Bad',\n",
                "    'Desolation', 'Darkness', 'Suffering', 'Helplessness', 'Devastated', 'Overwhelmed',\n",
                "    'Anxiety', 'Apprehensive', 'Pressure', 'Exhaustion', 'Numbness', 'Melancholy',\n",
                "    'Pensive', 'Obstacle', 'Miscalculation', 'Intimidation', 'Dismissive',\n",
                "    'EmotionalStorm', 'Mischievous',\n",
                "}\n",
                "\n",
                "NEUTRAL_EMOTIONS = {\n",
                "    'Neutral', 'Indifference', 'Nostalgia', 'Reflection', 'Contemplation',\n",
                "    'Ambivalence', 'Surprise', 'Confusion', 'Suspense', 'Yearning', 'Solitude',\n",
                "    'Coziness', 'Embarrassed', 'Embarrassment',\n",
                "}\n",
                "\n",
                "def map_sentiment(sent):\n",
                "    if sent in POSITIVE_EMOTIONS:\n",
                "        return 'Positivo'\n",
                "    elif sent in NEGATIVE_EMOTIONS:\n",
                "        return 'Negativo'\n",
                "    elif sent in NEUTRAL_EMOTIONS:\n",
                "        return 'Neutro'\n",
                "    else:\n",
                "        pos_kw = ['joy', 'happy', 'love', 'hope', 'good', 'great', 'excit', 'wonder',\n",
                "                  'posit', 'glad', 'cheer', 'bright', 'amaz', 'thrill', 'bliss']\n",
                "        neg_kw = ['sad', 'bad', 'hate', 'fear', 'angry', 'angr', 'grief', 'depress',\n",
                "                  'negat', 'despair', 'pain', 'sorrow', 'hurt', 'rage', 'bitter']\n",
                "        s_lower = sent.lower()\n",
                "        for kw in pos_kw:\n",
                "            if kw in s_lower:\n",
                "                return 'Positivo'\n",
                "        for kw in neg_kw:\n",
                "            if kw in s_lower:\n",
                "                return 'Negativo'\n",
                "        return 'Neutro'\n",
                "\n",
                "df['sentiment_group'] = df['Sentiment'].apply(map_sentiment)\n",
                "print('Distribuci√≥n de clases:')\n",
                "print(df['sentiment_group'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "features_cell"
            },
            "outputs": [],
            "source": [
                "le = LabelEncoder()\n",
                "y = le.fit_transform(df['sentiment_group'])\n",
                "print(f'Clases: {le.classes_}')\n",
                "\n",
                "num_features = ['Retweets', 'Likes', 'Year', 'Month', 'Day', 'Hour']\n",
                "X_num  = df[num_features].fillna(0).values.astype(np.float32)\n",
                "X_text = df['Text'].values\n",
                "\n",
                "# Divisi√≥n 70/15/15\n",
                "X_text_train, X_text_temp, X_num_train, X_num_temp, y_train, y_temp = train_test_split(\n",
                "    X_text, X_num, y, test_size=0.30, random_state=RANDOM_STATE, stratify=y\n",
                ")\n",
                "X_text_val, X_text_test, X_num_val, X_num_test, y_val, y_test = train_test_split(\n",
                "    X_text_temp, X_num_temp, y_temp, test_size=0.50, random_state=RANDOM_STATE, stratify=y_temp\n",
                ")\n",
                "\n",
                "# TF-IDF (solo en train)\n",
                "MAX_FEATURES = 100  # Reducido para minimizar par√°metros del modelo\n",
                "tfidf = TfidfVectorizer(max_features=MAX_FEATURES, ngram_range=(1, 1), sublinear_tf=True)\n",
                "X_tfidf_train = tfidf.fit_transform(X_text_train).toarray().astype(np.float32)\n",
                "X_tfidf_val   = tfidf.transform(X_text_val).toarray().astype(np.float32)\n",
                "X_tfidf_test  = tfidf.transform(X_text_test).toarray().astype(np.float32)\n",
                "\n",
                "# Combinar TF-IDF + num√©ricas\n",
                "X_train_np = np.hstack([X_tfidf_train, X_num_train])  # (N_train, 100+6=106)\n",
                "X_val_np   = np.hstack([X_tfidf_val,   X_num_val])\n",
                "X_test_np  = np.hstack([X_tfidf_test,  X_num_test])\n",
                "\n",
                "INPUT_DIM = X_train_np.shape[1]\n",
                "NUM_CLASSES = len(le.classes_)\n",
                "print(f'Train: {len(y_train)} | Val: {len(y_val)} | Test: {len(y_test)}')\n",
                "print(f'Dimensi√≥n de entrada: {INPUT_DIM}  |  N¬∫ clases: {NUM_CLASSES}')\n",
                "\n",
                "# Calcular pesos de clase para tratar el desbalanceo\n",
                "weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
                "class_weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)\n",
                "print(f'Pesos de clase calculados: {dict(zip(le.classes_, weights))}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3) Construcci√≥n del modelo (desde models/simple_nn.py)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "build_model_cell"
            },
            "outputs": [],
            "source": [
                "# Cargar modelo desde el .py\n",
                "HIDDEN_DIM = 64\n",
                "model = build_model(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES)\n",
                "model = model.to(device)\n",
                "\n",
                "n_params = model.count_parameters()\n",
                "print(model)\n",
                "print(f'\\n N√∫mero total de par√°metros entrenables: {n_params:,}')\n",
                "print(f'  fc1: {INPUT_DIM} x {HIDDEN_DIM} + {HIDDEN_DIM} = {INPUT_DIM*HIDDEN_DIM + HIDDEN_DIM}')\n",
                "print(f'  fc2: {HIDDEN_DIM} x {NUM_CLASSES} + {NUM_CLASSES} = {HIDDEN_DIM*NUM_CLASSES + NUM_CLASSES}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4) Entrenamiento"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "dataloaders_cell"
            },
            "outputs": [],
            "source": [
                "# Preparar tensores y DataLoaders\n",
                "def to_tensors(X, y):\n",
                "    return TensorDataset(\n",
                "        torch.tensor(X, dtype=torch.float32),\n",
                "        torch.tensor(y, dtype=torch.long)\n",
                "    )\n",
                "\n",
                "BATCH_SIZE = 32\n",
                "\n",
                "train_dataset = to_tensors(X_train_np, y_train)\n",
                "val_dataset   = to_tensors(X_val_np,   y_val)\n",
                "test_dataset  = to_tensors(X_test_np,  y_test)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE)\n",
                "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE)\n",
                "\n",
                "print(f'Batches en train: {len(train_loader)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train_loop_cell"
            },
            "outputs": [],
            "source": [
                "EPOCHS = 100\n",
                "LR     = 1e-3\n",
                "\n",
                "# Usar los pesos de clase en la funci√≥n de p√©rdida\n",
                "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
                "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
                "\n",
                "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
                "\n",
                "print(f'Entrenando por {EPOCHS} √©pocas...\\n')\n",
                "for epoch in range(1, EPOCHS + 1):\n",
                "    # --- Training ---\n",
                "    model.train()\n",
                "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
                "    for X_batch, y_batch in train_loader:\n",
                "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(X_batch)\n",
                "        loss = criterion(outputs, y_batch)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        train_loss    += loss.item() * len(y_batch)\n",
                "        preds          = outputs.argmax(dim=1)\n",
                "        train_correct += (preds == y_batch).sum().item()\n",
                "        train_total   += len(y_batch)\n",
                "\n",
                "    # --- Validation ---\n",
                "    model.eval()\n",
                "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
                "    with torch.no_grad():\n",
                "        for X_batch, y_batch in val_loader:\n",
                "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "            outputs   = model(X_batch)\n",
                "            loss      = criterion(outputs, y_batch)\n",
                "            val_loss += loss.item() * len(y_batch)\n",
                "            preds      = outputs.argmax(dim=1)\n",
                "            val_correct += (preds == y_batch).sum().item()\n",
                "            val_total   += len(y_batch)\n",
                "\n",
                "    t_loss = train_loss / train_total\n",
                "    v_loss = val_loss   / val_total\n",
                "    t_acc  = train_correct / train_total\n",
                "    v_acc  = val_correct   / val_total\n",
                "\n",
                "    history['train_loss'].append(t_loss)\n",
                "    history['val_loss'].append(v_loss)\n",
                "    history['train_acc'].append(t_acc)\n",
                "    history['val_acc'].append(v_acc)\n",
                "\n",
                "    if epoch % 10 == 0 or epoch == 1:\n",
                "        print(f'√âpoca {epoch:3d}/{EPOCHS}  |  '\n",
                "              f'Train Loss: {t_loss:.4f}  Train Acc: {t_acc:.4f}  |  '\n",
                "              f'Val Loss: {v_loss:.4f}  Val Acc: {v_acc:.4f}')\n",
                "\n",
                "print('\\nEntrenamiento completado.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5) Curvas de entrenamiento"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "curves_cell"
            },
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Loss\n",
                "axes[0].plot(history['train_loss'], label='Train Loss', color='steelblue')\n",
                "axes[0].plot(history['val_loss'],   label='Val Loss',   color='orange')\n",
                "axes[0].set_title('Curva de p√©rdida (Loss)')\n",
                "axes[0].set_xlabel('√âpoca')\n",
                "axes[0].set_ylabel('Loss (CrossEntropy)')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Accuracy\n",
                "axes[1].plot(history['train_acc'], label='Train Acc', color='steelblue')\n",
                "axes[1].plot(history['val_acc'],   label='Val Acc',   color='orange')\n",
                "axes[1].set_title('Curva de exactitud (Accuracy)')\n",
                "axes[1].set_xlabel('√âpoca')\n",
                "axes[1].set_ylabel('Accuracy')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.suptitle('Curvas de entrenamiento - Red Neuronal Simple (MLP)', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f'\\nConvergencia final:')\n",
                "print(f'  Train Loss: {history[\"train_loss\"][-1]:.4f} | Val Loss: {history[\"val_loss\"][-1]:.4f}')\n",
                "print(f'  Train Acc : {history[\"train_acc\"][-1]:.4f} | Val Acc : {history[\"val_acc\"][-1]:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6) Evaluaci√≥n en Train, Validaci√≥n y Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "evaluate_cell"
            },
            "outputs": [],
            "source": [
                "def evaluate_nn(model, loader, split_name, class_names, device):\n",
                "    model.eval()\n",
                "    all_preds, all_labels = [], []\n",
                "    with torch.no_grad():\n",
                "        for X_batch, y_batch in loader:\n",
                "            X_batch = X_batch.to(device)\n",
                "            outputs = model(X_batch)\n",
                "            preds   = outputs.argmax(dim=1).cpu().numpy()\n",
                "            all_preds.extend(preds)\n",
                "            all_labels.extend(y_batch.numpy())\n",
                "\n",
                "    acc = accuracy_score(all_labels, all_preds)\n",
                "    f1  = f1_score(all_labels, all_preds, average='macro')\n",
                "    print(f'\\n=== {split_name} ===')\n",
                "    print(f'Accuracy : {acc:.4f}')\n",
                "    print(f'F1-Macro : {f1:.4f}')\n",
                "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
                "    return acc, f1, np.array(all_labels), np.array(all_preds)\n",
                "\n",
                "class_names = le.classes_\n",
                "\n",
                "acc_train, f1_train, _, _                         = evaluate_nn(model, train_loader, 'TRAIN',      class_names, device)\n",
                "acc_val,   f1_val,   _, _                         = evaluate_nn(model, val_loader,   'VALIDACI√ìN', class_names, device)\n",
                "acc_test,  f1_test,  y_true_test, y_pred_test_arr = evaluate_nn(model, test_loader,  'TEST',       class_names, device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7) Matriz de confusi√≥n (Test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "confusion_cell"
            },
            "outputs": [],
            "source": [
                "cm = confusion_matrix(y_true_test, y_pred_test_arr)\n",
                "\n",
                "plt.figure(figsize=(6, 5))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n",
                "            xticklabels=class_names, yticklabels=class_names)\n",
                "plt.title('Matriz de Confusi√≥n - Red Neuronal Simple (Test)')\n",
                "plt.ylabel('Real')\n",
                "plt.xlabel('Predicho')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8) Resumen de Resultados"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "summary_cell"
            },
            "outputs": [],
            "source": [
                "results = pd.DataFrame({\n",
                "    'Split':    ['Train', 'Validaci√≥n', 'Test'],\n",
                "    'Accuracy': [acc_train, acc_val, acc_test],\n",
                "    'F1-Macro': [f1_train,  f1_val,  f1_test]\n",
                "})\n",
                "results = results.round(4)\n",
                "\n",
                "print('\\n=== RESUMEN RED NEURONAL SIMPLE ===')\n",
                "print(f'Arquitectura     : Linear({INPUT_DIM}, {HIDDEN_DIM}) ‚Üí ReLU ‚Üí Linear({HIDDEN_DIM}, {NUM_CLASSES})')\n",
                "print(f'N√∫mero de par√°metros : {n_params:,}')\n",
                "print(results.to_string(index=False))\n",
                "\n",
                "print('\\nüìã Tabla para el README:')\n",
                "print(f'| Simple MLP | {n_params:,} | {acc_train:.4f} | {acc_val:.4f} | {acc_test:.4f} | {f1_train:.4f} | {f1_val:.4f} | {f1_test:.4f} |')"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.14.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}